{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kickstarter.com scraper\n",
    "Perry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #html\n",
    "from bs4 import BeautifulSoup #scraping\n",
    "import re #regex\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Get & save project URLs to scrape\n",
    "\n",
    "- Finds project URLs from the tabletop games category explore page\n",
    "- Checks if kick_urls.txt exists, if so then pulls in any additional urls not scraped this run  \n",
    "- Saves to kick_urls.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_urls(explore_url):\n",
    "    '''\n",
    "    Scrapes project page URLs from explore_url\n",
    "    Input:\n",
    "        explore_url: kickstarter explore page, must end with page variable open (i.e. \"&page=\")\n",
    "        Also reads from kick_urls.txt\n",
    "    Output:\n",
    "        project_urls: set of url strings found by iterating through the explore page\n",
    "    '''\n",
    "    #Regex to grab project links. Format: [...]kickstarter.com/projects/[creator]/[project]\n",
    "    project_url_regex = re.compile(r\"https://www.kickstarter.com/projects/[0-9A-Za-z\\-]*/[0-9A-Za-z\\-]*\")\n",
    "    project_urls = set()\n",
    "\n",
    "    print(\"starting to scrape:\", explore_url)\n",
    "    \n",
    "    empty_count = 0\n",
    "\n",
    "    #Grab all project urls for each explore page.\n",
    "    #  Note: Kickstarter only generates up to 200 pages of history (~2400 projects)\n",
    "    for page in range(1,201):\n",
    "        response = requests.get(explore_url + str(page))\n",
    "        if response.status_code == 200:\n",
    "            new_urls = set(project_url_regex.findall(response.text))\n",
    "            \n",
    "            #increment count if page is empty\n",
    "            if len(new_urls.difference(project_urls)) == 0:\n",
    "                empty_count += 1\n",
    "            else:\n",
    "                empty_count = 0\n",
    "                \n",
    "            project_urls.update(new_urls)\n",
    "            \n",
    "            #progress tracker\n",
    "            if page % 10 == 0:\n",
    "                print(\"Finished page\", page)\n",
    "        else:\n",
    "            print(\"Page\",page,\"fail. Status code:\",response.status_code,\"Reason:\",response.reason)\n",
    "        \n",
    "        #If we are getting empty pages (no more projects)\n",
    "        if empty_count >= 5:\n",
    "            print(str(empty_count), \"consecutive pages with no new project urls. Quitting scrape\")\n",
    "            break\n",
    "\n",
    "    print (str(len(project_urls)),\"distinct project urls scraped\")\n",
    "    return project_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to scrape: https://www.kickstarter.com/discover/advanced?state=successful&category_id=34&woe_id=23424977&sort=end_date&seed=2528671&page=\n",
      "Finished page 10\n",
      "Finished page 20\n",
      "Finished page 30\n",
      "Finished page 40\n",
      "Finished page 50\n",
      "Finished page 60\n",
      "Finished page 70\n",
      "Finished page 80\n",
      "Finished page 90\n",
      "Finished page 100\n",
      "Finished page 110\n",
      "Finished page 120\n",
      "Finished page 130\n",
      "Finished page 140\n",
      "Finished page 150\n",
      "Finished page 160\n",
      "Finished page 170\n",
      "Finished page 180\n",
      "Finished page 190\n",
      "Finished page 200\n",
      "2420 distinct project urls scraped\n",
      "Combining... 2420 distinct urls\n",
      "starting to scrape: https://www.kickstarter.com/discover/advanced?state=live&category_id=34&woe_id=23424977&raised=2&sort=end_date&seed=2528671&page=\n",
      "Finished page 10\n",
      "5 consecutive pages with no new project urls. Quitting scrape\n",
      "81 distinct project urls scraped\n",
      "Combining... 2486 distinct urls\n",
      "starting to scrape: https://www.kickstarter.com/discover/advanced?category_id=34&woe_id=23424977&raised=1&sort=end_date&seed=2528671&page=\n",
      "Finished page 10\n",
      "5 consecutive pages with no new project urls. Quitting scrape\n",
      "178 distinct project urls scraped\n",
      "Combining... 2646 distinct urls\n",
      "starting to scrape: https://www.kickstarter.com/discover/advanced?category_id=34&woe_id=23424977&raised=0&sort=end_date&seed=2528671&page=\n",
      "Finished page 10\n",
      "Finished page 20\n",
      "Finished page 30\n",
      "Finished page 40\n",
      "Finished page 50\n",
      "Finished page 60\n",
      "Finished page 70\n",
      "Finished page 80\n",
      "Finished page 90\n",
      "Finished page 100\n",
      "Finished page 110\n",
      "Finished page 120\n",
      "Finished page 130\n",
      "Finished page 140\n",
      "Finished page 150\n",
      "Finished page 160\n",
      "Finished page 170\n",
      "Finished page 180\n",
      "Finished page 190\n",
      "Finished page 200\n",
      "2413 distinct project urls scraped\n",
      "Combining... 5044 distinct urls\n",
      "5097 distinct project urls after reading archive\n"
     ]
    }
   ],
   "source": [
    "# Specify explore pages to scrape for project URLs\n",
    "# Universal filters: Tabletop Games, US based, sort on end date\n",
    "#  1) Successful:    (>100% funded & closed) [~5,800]\n",
    "#  2) Live [>100%]:  (>100% funded & live) [~64]\n",
    "#  3) All [75-100%]: (75-100% live or closed) [~165 of which ~13 live]\n",
    "#  4) All [<75%):    (0-75% live or closed) [~4,400 of which ~49 live]\n",
    "explore_urls = [\"https://www.kickstarter.com/discover/advanced?state=successful&category_id=34&woe_id=23424977&sort=end_date&seed=2528671&page=\", \n",
    "                 \"https://www.kickstarter.com/discover/advanced?state=live&category_id=34&woe_id=23424977&raised=2&sort=end_date&seed=2528671&page=\",\n",
    "                 \"https://www.kickstarter.com/discover/advanced?category_id=34&woe_id=23424977&raised=1&sort=end_date&seed=2528671&page=\",\n",
    "                 \"https://www.kickstarter.com/discover/advanced?category_id=34&woe_id=23424977&raised=0&sort=end_date&seed=2528671&page=\"\n",
    "                ]\n",
    "\n",
    "project_urls = set()\n",
    "save_file_path = \"kick_urls.txt\"\n",
    "\n",
    "#Scrape the URL pages\n",
    "for url in explore_urls:\n",
    "    project_urls.update(scrape_urls(url))\n",
    "    print(\"Combining...\",str(len(project_urls)),\"distinct urls\")\n",
    "\n",
    "#Check previous save file (if exists) for projects we didn't scrape this time\n",
    "try:\n",
    "    with open(save_file_path,\"r\") as f:\n",
    "        for line in f:\n",
    "            project_urls.add(line.strip())\n",
    "    print (str(len(project_urls)),\"distinct project urls after reading archive\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No pre-existing file - skipping import\")\n",
    "\n",
    "#Write project urls to save file\n",
    "with open(save_file_path,\"w\") as f:\n",
    "    f.writelines([url + \"\\n\" for url in project_urls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Scrape individual project pages\n",
    "\n",
    "Things to get:\n",
    "- Dependent var  \n",
    "  - num backers  \n",
    "  - total $ funded  \n",
    "  - % of goal raised (which will always be >100%+ because kickstarter doesn't surface \"failed\" projects)\n",
    "\n",
    "- Independent vars  \n",
    "  - Funding goal (dollars)\n",
    "  - Rewards: # of reward levels, min reward cost, max reward cost  \n",
    "  - Funding period: campaign start date, campaign end date, campaign date\n",
    "  - Project activity: # FAQ posts, # project update posts\n",
    "  - Backers: % from US, % new backers vs. have backed before\n",
    "  - Creator: # projects created, # projects backed  \n",
    "\n",
    "Output to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num(string):\n",
    "    '''\n",
    "    Input: string\n",
    "    Output: first valid int found in string with comma formatting removed\n",
    "    Error if no int found\n",
    "    '''\n",
    "    num_match = re.search(r\"[0-9,]+\",string)\n",
    "    num_text = num_match.group(0).replace(\",\",\"\")\n",
    "    return int(num_text)\n",
    "\n",
    "def get_html(url, attempts=3, pause=2, rate_limit=0):\n",
    "    '''\n",
    "    Input: \n",
    "        url: url address string\n",
    "        attempts: # tries (default = 3)\n",
    "        pause: # seconds pause after failed request (default = 2)\n",
    "        rate_limit: # seconds pause before every request (default = 0)\n",
    "    Output: (status, html)\n",
    "        status: Status code \n",
    "        html: html string\n",
    "    '''\n",
    "    for i in range(attempts):\n",
    "        if (rate_limit > 0):\n",
    "            time.sleep(rate_limit)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return (response.status_code, response.text)\n",
    "        else:\n",
    "            print (\"Request failed with code:\", str(response.status_code), str(i+1), \"attempt out of\", str(attempts))\n",
    "            if (i<attempts) and (pause>0):\n",
    "                time.sleep(pause)\n",
    "    return(response.status_code, response.text)\n",
    "\n",
    "def make_row(url, data, data_headers):\n",
    "    '''\n",
    "    Returns a data row as a list for csv.writer to output\n",
    "    Inputs:\n",
    "        url: url string (first column)\n",
    "        data: dict {data_headers -> values}\n",
    "        data_headers: list of column names in the desired csv order\n",
    "    '''\n",
    "    line = [url]\n",
    "    line.extend([data[x] for x in data_headers])\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main scraping function\n",
    "def scrape_project(url, html):\n",
    "    data = {}\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    \n",
    "    #Project name\n",
    "    data[\"project_name\"] = soup.find(\"meta\",attrs={\"property\":\"og:title\"})[\"content\"].strip()\n",
    "    \n",
    "    #Success metrics - total $ pledged, goal $, num backers\n",
    "    result = soup.find(\"div\",class_=\"col-right col-4 py3 border-left\")\n",
    "    if (result):\n",
    "        data[\"dollar_total\"] = get_num(result.find(\"h3\",class_=\"mb0\").span.text)\n",
    "        data[\"dollar_goal\"] = get_num(result.find(\"div\",class_=\"type-12 medium navy-500\").span.text)\n",
    "        data[\"backers_total\"] = get_num(result.find(\"div\",class_=\"mb0\").h3.text)\n",
    "    else:\n",
    "        result = soup.find(\"div\",id=\"pledged\")\n",
    "        data[\"dollar_total\"] = get_num(result[\"data-pledged\"])\n",
    "        data[\"dollar_goal\"] = get_num(result[\"data-goal\"])\n",
    "        data[\"backers_total\"] = get_num(soup.find(\"div\",id=\"backers_count\")[\"data-backers-count\"])\n",
    "        \n",
    "    #Reward tiers - number of tiers, reward tier costs / popularity\n",
    "    result = soup.find_all(\"div\",class_=\"pledge__info\")\n",
    "    rewards = []\n",
    "    reward_mode_cost = -1\n",
    "    reward_mode_backers = -1\n",
    "    \n",
    "    for element in result:\n",
    "        e2 = element.find(\"h2\",class_=\"pledge__amount\").span\n",
    "        #On live campaigns skip the \"pledge without rewards\" option that breaks the find\n",
    "        if (e2 is None):\n",
    "            continue\n",
    "        reward_cost = get_num(e2.text)\n",
    "        rewards.append(reward_cost)\n",
    "        \n",
    "        #Track the most popular reward tier (likely the \"base\" tier that gets you 1 copy of the game/product)\n",
    "        reward_backers = get_num(element.find(\"span\",class_=\"pledge__backer-count\").text)\n",
    "        if (reward_backers > reward_mode_backers):\n",
    "            reward_mode_cost = reward_cost\n",
    "            reward_mode_backers = reward_backers\n",
    "    \n",
    "    data[\"reward_count\"] = len(result)\n",
    "    data[\"reward_min\"] = np.min(rewards)\n",
    "    data[\"reward_max\"] = np.max(rewards)\n",
    "    data[\"reward_mean\"] = np.mean(rewards)\n",
    "    data[\"reward_median\"] = np.median(rewards)\n",
    "    data[\"reward_mode_cost\"] = reward_mode_cost\n",
    "    data[\"reward_mode_backers\"] = reward_mode_backers\n",
    "    \n",
    "    #Funding period - days open, start date, close date\n",
    "    result = soup.find(\"div\",class_=\"NS_campaigns__funding_period\")\n",
    "    if (result):\n",
    "        element = result.time\n",
    "        data[\"funding_start\"] = element[\"datetime\"].split(\"T\")[0].strip()\n",
    "        element = element.nextSibling.nextSibling\n",
    "        data[\"funding_end\"] = element[\"datetime\"].split(\"T\")[0].strip()\n",
    "        element = element.nextSibling #this element is :NavigableString (not :Tag) so doesn't have .text attribute\n",
    "        data[\"funding_days\"] = get_num(element)\n",
    "    else:\n",
    "        #Incompete projects don't show start date. If necessary: fill from updates then back-calc funding_days\n",
    "        data[\"funding_start\"] = \"Not found\"\n",
    "        data[\"funding_end\"] = soup.find(\"p\",class_=\"mb3 mb0-lg type-12\").time[\"datetime\"].split(\"T\")[0].strip()\n",
    "        data[\"funding_days\"] = -1\n",
    "    \n",
    "    # Project activity - count of FAQ / update / comment entries\n",
    "    #--FAQ\n",
    "    result = soup.find(\"a\",attrs={\"data-content\":\"faqs\"}).find(\"span\",class_=\"count\")\n",
    "    if (result):\n",
    "        data[\"activity_faq_total\"] = get_num(result.text)\n",
    "    else:\n",
    "        data[\"activity_faq_total\"] = 0\n",
    "    #--Updates\n",
    "    result = soup.find(\"a\",attrs={\"data-content\":\"updates\"}).find(\"span\",class_=\"count\")\n",
    "    if (result):\n",
    "        data[\"activity_update_total\"] = get_num(result.text)\n",
    "    else:\n",
    "        data[\"activity_update_total\"] = 0\n",
    "    #--Comments\n",
    "    result = soup.find(\"a\",attrs={\"data-content\":\"comments\"}).find(\"span\",class_=\"count\")\n",
    "    if (result):\n",
    "        data[\"activity_comment_total\"] = get_num(result.text)\n",
    "    else:\n",
    "        data[\"activity_update_total\"] = 0\n",
    "        \n",
    "    #--FAQ before funding end date\n",
    "    code, text = get_html(url + \"/faqs\")\n",
    "    if code == 200:\n",
    "        faq_count = 0\n",
    "        soup = BeautifulSoup(text, \"lxml\")\n",
    "        result = soup.find_all(\"time\")\n",
    "        for element in result:\n",
    "            date = element[\"datetime\"].split(\"T\")[0].strip()\n",
    "            if date <= data[\"funding_end\"]:\n",
    "                faq_count += 1\n",
    "        data[\"activity_faq_end\"] = faq_count        \n",
    "    else:\n",
    "        print (\"Subrequest failed - /faqs ->\",str(code),url)\n",
    "        data[\"activity_faq_end\"] = -1\n",
    "    \n",
    "    #--Updates before funding end date\n",
    "    code, text = get_html(url + \"/updates\")\n",
    "    if code == 200:\n",
    "        update_count = 0\n",
    "        result = []\n",
    "        soup = BeautifulSoup(text, \"lxml\")\n",
    "        \n",
    "        if data[\"funding_start\"] == \"Not found\":\n",
    "            data[\"funding_start\"] = soup.find(\"div\",class_=\"timeline__divider--launched\").time[\"datetime\"].split(\"T\")[0].strip()\n",
    "        \n",
    "        #Look for a success / failure banner in the updates timeline\n",
    "        #If one exists, only count entries below the banner. Otherwise count all entries\n",
    "        success_divider = soup.find(class_=\"timeline__divider timeline__divider--successful\")\n",
    "        fail_divider = soup.find(class_=\"timeline__divider timeline__divider--failure\")\n",
    "        if (success_divider):\n",
    "            result = success_divider.find_all_next(\"p\",class_=\"grid-post__date\")\n",
    "        elif (fail_divider):\n",
    "            result = fail_divider.find_all_next(\"p\",class_=\"grid-post__date\")\n",
    "        else:\n",
    "            result = soup.find_all(\"p\",class_=\"grid-post__date\")\n",
    "        \n",
    "        for element in result:\n",
    "            date = element.time[\"datetime\"].split(\"T\")[0].strip()\n",
    "            if date <= data[\"funding_end\"]:\n",
    "                update_count += 1\n",
    "        data[\"activity_update_end\"] = update_count        \n",
    "    else:\n",
    "        print (\"Subrequest failed - /updates ->\",str(code),url)\n",
    "        data[\"activity_update_end\"] = -1\n",
    "        \n",
    "    #--Comments before funding end date\n",
    "    #--Not pulling these because there are a lot (thousands) without a good option to scroll, \n",
    "    #  plus number of comments isn't really controlled by the creator\n",
    "    \n",
    "    #Backers - # from US, # new backers vs. have backed before\n",
    "    code, text = get_html(url + \"/community\",attempts=1)\n",
    "    if code == 200:\n",
    "        soup = BeautifulSoup(text, \"lxml\")\n",
    "        result = soup.find(\"div\",class_=\"community-section__locations_countries\")\n",
    "        if (result):\n",
    "            element = result.find(\"a\",text=\"United States\")\n",
    "            if (element):\n",
    "                element = result.find(\"a\",text=\"United States\").findNext(\"div\",class_=\"tertiary-text js-location-tertiary-text\")\n",
    "                data[\"backers_US\"] = get_num(element.text)\n",
    "            else:\n",
    "                data[\"backers_US\"] = -1\n",
    "        else:\n",
    "            data[\"backers_US\"] = -1\n",
    "        \n",
    "        result = soup.find(\"div\",class_=\"community-section__new_vs_existing\")\n",
    "        if(result):\n",
    "            element = result.find(\"div\",class_=\"new-backers\").find(\"div\",class_=\"count\")\n",
    "            data[\"backers_new\"] = get_num(element.text)\n",
    "            element = result.find(\"div\",class_=\"existing-backers\").find(\"div\",class_=\"count\")\n",
    "            data[\"backers_exist\"] = get_num(element.text)\n",
    "        else:\n",
    "            data[\"backers_new\"] = -1\n",
    "            data[\"backers_exist\"] = -1\n",
    "    else:\n",
    "        print (\"Subrequest failed - /community ->\",str(code),url)\n",
    "        data[\"backers_US\"] = -1\n",
    "        data[\"backers_new\"] = -1\n",
    "        data[\"backers_exist\"] = -1\n",
    "    \n",
    "    #Creator - # projects created, # projects backed\n",
    "    code, text = get_html(url + \"/creator_bio\")\n",
    "    if code == 200:\n",
    "        soup = BeautifulSoup(text, \"lxml\")\n",
    "        result = soup.find(\"div\",class_=\"created-projects py2 f5 mb3\")\n",
    "        \n",
    "        #Creator projects\n",
    "        #--Seems like this count includes projects created *after* the current one. \n",
    "        regex = re.compile(r\"([\\d\\w]+) created\")\n",
    "        created = regex.search(result.text.strip()).group(1)\n",
    "        if (created == \"First\"): #special case for first project\n",
    "            data[\"creator_projects\"] = 1\n",
    "        else:\n",
    "            data[\"creator_projects\"] = get_num(created)\n",
    "        \n",
    "        #Creator backed\n",
    "        regex = re.compile(r\"([\\d\\w]+) backed\")\n",
    "        backed = regex.search(result.text.strip()).group(1)\n",
    "        data[\"creator_backed\"] = get_num(backed)\n",
    "        \n",
    "    else:\n",
    "        print (\"Subrequest failed - /community ->\",str(code),url)\n",
    "        data[\"creator_projects\"] = -1\n",
    "        data[\"creator_backed\"] = -1\n",
    "        \n",
    "    return data\n",
    "\n",
    "#Temporary - scrape project name to backfill (didn't pull this originally)\n",
    "def scrape_name(url):\n",
    "    code, html = get_html(url)\n",
    "    if code == 200:\n",
    "        soup = BeautifulSoup(html,\"lxml\")\n",
    "        return soup.find(\"meta\",attrs={\"property\":\"og:title\"})[\"content\"].strip()\n",
    "    else:\n",
    "        return \" \"\n",
    "\n",
    "def scrape_fund_start(url):\n",
    "    code, html = get_html(url+\"/updates\")\n",
    "    if code == 200:\n",
    "        soup = BeautifulSoup(html,\"lxml\")\n",
    "        return soup.find(\"div\",class_=\"timeline__divider--launched\").time[\"datetime\"].split(\"T\")[0].strip()\n",
    "    else:\n",
    "        return \"Not found\"\n",
    "    \n",
    "#debugging\n",
    "def debug():\n",
    "    url = \"https://www.kickstarter.com/projects/1208693854/the-island-of-el-dorado\" #completed\n",
    "    #url = \"https://www.kickstarter.com/projects/ghoulash/ghoulash-the-card-game\"    #failed\n",
    "    #url = \"https://www.kickstarter.com/projects/1048213369/project-alpaca\"          #live\n",
    "\n",
    "    code, html = get_html(url)\n",
    "    scrape_project(url, html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver \n",
    "Scrapes URLs from text file, outputs to csv  \n",
    "To do:\n",
    "- Add code to grab URLs from unsuccessful projects (but avoid unfinished ones)\n",
    "- Add code to read in previous .csv and avoid rescraping unless error values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_save_path = \"kick_urls.txt\"\n",
    "project_save_path = \"kick_data.csv\"\n",
    "\n",
    "column_headers = ['url', 'project_name', 'activity_comment_total', 'activity_faq_end', 'activity_faq_total', \n",
    "                  'activity_update_end', 'activity_update_total', 'backers_US', 'backers_total', \n",
    "                  'backers_exist', 'backers_new', 'creator_backed', 'creator_projects', 'dollar_goal', \n",
    "                  'dollar_total','funding_days', 'funding_end', 'funding_start', 'reward_count', 'reward_max', \n",
    "                  'reward_mean', 'reward_median', 'reward_min','reward_mode_backers', 'reward_mode_cost']\n",
    "    \n",
    "#Read previous csv (if exists). Make dict {url -> {other columns:values}}\n",
    "data_dict = {}\n",
    "try:\n",
    "    with open(project_save_path,\"r\") as f_in:\n",
    "        #skip header row\n",
    "        f_in.readline()\n",
    "        #pull in data rows\n",
    "        reader = csv.reader(f_in, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            data_dict[row[0]]=dict(zip(column_headers[1:],row[1:]))\n",
    "    print (str(len(data_dict)),\"projects imported from archive\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No pre-existing save file - skipping import\")   \n",
    "\n",
    "#Get the urls from the first scraping step\n",
    "urls = []\n",
    "with open(url_save_path,\"r\") as f_in:\n",
    "    urls = [line.strip() for line in f_in] \n",
    "    \n",
    "#Second scraping step\n",
    "with open(project_save_path,\"w\") as f_out:\n",
    "\n",
    "    #Output CSV\n",
    "    out = csv.writer(f_out, delimiter=\",\")\n",
    "    \n",
    "    #Re-write header and previous values\n",
    "    out.writerow(column_headers)\n",
    "    for url in data_dict.keys():\n",
    "        #back-filling project name\n",
    "        if \"project_name\" not in data_dict[url]:\n",
    "            data_dict[url][\"project_name\"] = scrape_name(url)\n",
    "        if data_dict[url][\"funding_start\"] == \"Not found\":\n",
    "            data_dict[url][\"funding_start\"] = scrape_fund_start(url)        \n",
    "        #write out data\n",
    "        out.writerow(make_row(url,data_dict[url],column_headers[1:]))\n",
    "    \n",
    "    #Progress tracker \n",
    "    progress = 0\n",
    "    \n",
    "    print(\"Starting scrape\")\n",
    "    for url in urls:\n",
    "        #If we scraped this before, skip (already wrote to csv)\n",
    "        if url in data_dict:\n",
    "            progress += 1 \n",
    "            \n",
    "        #Else, scrape\n",
    "        else:\n",
    "            #Fetch html, scrape if success, print error if fail\n",
    "            code, text = get_html(url,attempts=10, pause=5)\n",
    "            if code == 200:\n",
    "                #print(str(datetime.datetime.now()), \"Scrape:\", url)\n",
    "                #scrape\n",
    "                data = scrape_project(url, text)\n",
    "                #write out data\n",
    "                out.writerow(make_row(url,data,column_headers[1:]))\n",
    "                progress += 1\n",
    "            else:\n",
    "                print(\"HTML request failed. Code:\", str(response.status_code), \"URL:\", line)\n",
    "\n",
    "        if (progress % (len(urls) // 100)) == 0:\n",
    "            print(\"****\",str(progress),\"Urls scraped out of\",str(len(urls)))\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kick_urls.txt\",\"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        print (line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
